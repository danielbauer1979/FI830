{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm4Zb/hg7lH/ijpgUnyLKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/FI830/blob/main/FI830_MachineLearningBackground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A quick and dirty intro to Machine Learning**"
      ],
      "metadata": {
        "id": "jSrDbjCCL9wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basics**"
      ],
      "metadata": {
        "id": "x5R5uqEePkZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic supervised machine learning amounts to \"learning\" a functional relationship between *outcomes* or *labels* $y_i$ and a vector of features $x_i = (x_{i1},x_{i2},...,x_{ip})$ as in:\n",
        "$$\n",
        "y_i = f(x_i) + …õ_i,\n",
        "$$\n",
        "where $\\epsilon$ is an error term originating from model error, measurement error, or fundamental/unexplained uncertainty and $f$ is the function we are seeking to learn."
      ],
      "metadata": {
        "id": "tUbk-OTLMRFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make it concrete, in the application we're going to look at below, $y_i$ will refer to a house price for some house $i$ and $x_i$ will refer to the characteristics of the house, i.e., the size of the house in square feet, the number of bedrooms, the number of bathrooms, the age of the house in years, etc."
      ],
      "metadata": {
        "id": "cbbBPiD7NI3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a typical machine learning application, we will have $N$ observations of feature-label combinations: $(y_i,x_i)_{i=1,...,N}$. In our example, imagine we have prices and all characteristics for $N=100,000$ houses. We then \"learn\" the relationship $f$ making certain assumptions about its functional form.\n",
        "\n",
        "Imagine for instance that $f$ is a parametric function with a parameter vector $\\beta$, so $f=f(x,\\beta)$. We then determine the parameter vector $\\hat{\\beta}$ that best fits our data: $y_i \\approx \\hat{y}_i = f(x_i,\\hat{\\beta})$. We typically determine this \"best\" parameter $\\hat{\\beta}$ vector by minimizing the distance between the real observation $y_i$ and our predicions $\\hat{y}_i$:\n",
        "$$\n",
        "\\text{minimize } \\sum_{i=1}^N d(y_i,\\hat{y}_i) =  \\sum_{i=1}^N d(y_i,f(x_i,\\beta)),\n",
        "$$\n",
        "where $d$ is a distance function. This process is called *fitting* the model. Often we use quadratic distance $d(a,b)=(a-b)^2$, which yields:\n",
        "$$\n",
        "\\text{minimize } \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 =  \\sum_{i=1}^N (y_i - f(x_i,\\beta))^2,\n",
        "$$\n",
        "which is called a least-squares problem. However, different distance functions can be chosen, depending on the situation.\n",
        "\n",
        "Sometimes we test out different functional relationships, e.g. more complex and less complex functions in a given class of functions. This process is called *tuning* or *selecting* the model"
      ],
      "metadata": {
        "id": "U6J0LM3KNxcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have our fitted model $\\hat{f}(x)= f(x,\\hat{\\beta})$, we can *predict* the outcome for a set of features $x_0$ for which we do not know the outcome: $\\hat{y}_0 = \\hat{f}(x_0) = f(x_0,\\hat{\\beta}).$"
      ],
      "metadata": {
        "id": "0FUTRnJbQZWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again going back to the example of house prices, once we fitted a model based on the $N$ house prices we have available, we can predict the price for a house that we're thinking about offering for sale. Such a predictive model may be helpful for realtors offering houses for sale or for consumers looking to evaluate the price for a house (think e.g. about the \"Zestimate\" on zillow.com)."
      ],
      "metadata": {
        "id": "NRdv5stLWD7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The OG of Prediction models: Linear Regression**"
      ],
      "metadata": {
        "id": "cFZo5GfRWpcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the simplest and most established predictive models is *linear regression*. Here, we assume that the function $f$ is a linear combination of the features, where the feature weights are the parameters:\n",
        "$$\n",
        "f(x_i,\\beta) = x_i \\, \\beta = \\beta_0 + \\beta_1\\,x_{i1} + \\beta_2\\,x_{i2} + ... + \\beta_p\\,x_{ip}.\n",
        "$$\n",
        "\n",
        "Hence, the model assumption in this case is:\n",
        "$$\n",
        "y_i = f(x_i) + …õ_i = x_i \\, \\beta + …õ_i= \\beta_0 + \\beta_1\\,x_{i1} + \\beta_2\\,x_{i2} + ... + \\beta_p\\,x_{ip} + …õ_i,\n",
        "$$\n",
        "and the least-squares problem is:\n",
        "$$\n",
        "\\text{minimize } \\sum_{i=1}^N (y_i - x_i\\,\\beta)^2 = \\sum_{i=1}^N (y_i - (\\beta_0 + \\beta_1\\,x_{i1} + \\beta_2\\,x_{i2} + ... + \\beta_p\\,x_{ip}))^2.\n",
        "$$\n",
        "The solution to this problem is given by the well-known *ordinary least squares* (OLS) estimator.\n",
        "\n",
        "\"Model selection/tuning\" in the linear model essentially boils down to selecting features, i.e., to determine which attributes/features $x_{ij}$ should be included in the predictive model. Should we incorporate all available information or only those features that have substantial predictive power?  "
      ],
      "metadata": {
        "id": "Lao9mgZMW0LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Selection: In-sample vs. out-of-sample prediction**"
      ],
      "metadata": {
        "id": "7xnx2mfIcpxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A key point in predictive modeling is in-sample versus out-of-sample performance. A model may have a good in-sample performance (for instance, if we are using many parameters), but it may be \"overfitting\" the data so that the model's peformance is poor when applied to observations that are \"out-of-sample\", i.e., observations that were not used in the fitting process."
      ],
      "metadata": {
        "id": "pMt5XjuidN3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To appraise how a model performs out-of-sample, in machine learning applications one often splits the whole available dataset randomly into a so-called *training* set and a so-called *test* set. For instance, one may use 70% of the data for \"training\" and 30% of the data for \"testing\". The idea is then that the model is fit using the training data, but we can tune or select the best model out of several models by comparing the performance within the test set."
      ],
      "metadata": {
        "id": "WdXlq1H9fkCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concretely in the context of linear models you can think of different selection of features (for instance, by eliminating features that are not statistically significant). We can run models with different subsets of features, for instance more features (more complex models) and less featuresn (simpler models). By fitting them all to the training data, we obtain different models--and, hence, different predictions for each of these models. We can now compare the different models using the test set, i.e., by computing:\n",
        "$$\n",
        "Error_{test} = \\sum_{i \\in\\, Test Set} d(y_i,\\hat{y}_i).\n",
        "$$"
      ],
      "metadata": {
        "id": "H1HYE_ijjCOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This leads to the following \"cookbook recipe\" for building machine learning models."
      ],
      "metadata": {
        "id": "qGV7oCRykTF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Recipe**"
      ],
      "metadata": {
        "id": "AblTW9x6e72L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Given dataset of features and targets $(ùë•_ùëñ,ùë¶_ùëñ)$, $ùëñ=1,...,N$\n",
        "* Split your dataset into a training set and test set (at random).\n",
        "* Select a class of models (e.g linear regression models)\n",
        "* Fit each model to your training data by finding parameters of model that yield predictions $\\hat{y}_i$ for observation-feature ($(y_i,x_i)$) pairs that are \"close to\" the observation/targets $y_i$.\n",
        "* Compare the prediction error using the samples in the test set (that were not used for fitting).\n",
        "* Pick the model with the smallest test error\n",
        "* Use this model for prediction."
      ],
      "metadata": {
        "id": "juaJ7RtrdkyS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beyond Linear Regression Models: Random Forest**"
      ],
      "metadata": {
        "id": "cRgXAE92kdQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression models are relatively simple predictive models. They still are useful in many situations, e.g. when trying to understand causal relationships. Yet, for purely trying to build a model that is good at predicting, with the availability of fast computational resources, we often can do better than linear regression models."
      ],
      "metadata": {
        "id": "hiCLI8n7kktp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially, you can think about more advanced models as simply more complex functions $f(x,\\beta)$ that are used in the fitting problem that still have desirable mathematical properties so that fitting is fast. For instance, withing \"deep learning\", we are using so-called artificial neural networks that are model the working of brains."
      ],
      "metadata": {
        "id": "rmhMlJ-clM5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One class of predictive models that usually perform pretty well and are fairly robust in the building and fitting stage (i.e., it does not take a lot of expertise to work with them) are so-called *random forests*. You can think about the $f$ in a random forest as being combination of smaller $f$-s that are all given by decision trees. The key \"tuning\" parameters that one needs to set is how many individual trees one wants to use (let's call it $M$) and how large each of the individual trees is (let's call in $D$)."
      ],
      "metadata": {
        "id": "HsqsGRmzl0Xr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A Worked Example: Predicting House Prices**"
      ],
      "metadata": {
        "id": "K6uTxWU0mldx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build a predictive model for house prices just as we discussed above. So $y_i$-s are house prices and we will have a bunch of features for each house."
      ],
      "metadata": {
        "id": "p2NMAp35oBO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load some helpful libraries. First, let's load some basic py libraries that are going to be generically helpful:"
      ],
      "metadata": {
        "id": "vyGzIzCHnQfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jWEdVjYnNGNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the machine learning model, we are going to rely on the python sklearn library. We will explain the functionality as we go forward."
      ],
      "metadata": {
        "id": "FRJIGgErnuqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import sklearn.metrics as metrics\n"
      ],
      "metadata": {
        "id": "Az8zPPcnntKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we will use is available in the github folder for my machine learning class, so we load it from there:"
      ],
      "metadata": {
        "id": "6WzkFQoKoS0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git\n",
        "house = pd.read_csv('ML_656/HouseData.csv')"
      ],
      "metadata": {
        "id": "NWzZgGYoobMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the first few lines:"
      ],
      "metadata": {
        "id": "UETUV02GojyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house.head()"
      ],
      "metadata": {
        "id": "OwfcUxPlonNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So clearly we will try to predict \"SalePrice\" that is given in the last column (that's our $y_i$ where each line is an observation $i$) using the information in the remaing columns--that's our feature vector."
      ],
      "metadata": {
        "id": "LFzMaCAKoqO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do some basic data preparation by deleting columns with missing data and deleting records with missing data."
      ],
      "metadata": {
        "id": "6ZNUWm5ZpBdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house = house.drop(columns=['Id','LotFrontage','Alley', 'BsmtQual', 'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature'])\n",
        "house = house.dropna()"
      ],
      "metadata": {
        "id": "ZBomHshEpdGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to convert the categorical variables into dummies:"
      ],
      "metadata": {
        "id": "CcDoFAiCrCbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerics = list(house.select_dtypes(include=['int64']).columns)\n",
        "factors = list(house.select_dtypes(include=['object']).columns)\n",
        "house_numcols = house[numerics]\n",
        "house_faccols = house[factors]\n",
        "dummies = pd.get_dummies(house_faccols, drop_first=True)\n",
        "house = pd.concat([house_numcols, dummies], axis = 1)\n",
        "house.head()"
      ],
      "metadata": {
        "id": "Ic2JCeTprHYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to build a random forest model. We first randomly split the model into a training and a test data set, and isolating the features from the observations."
      ],
      "metadata": {
        "id": "LrFgRq-ap2T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "train, test = train_test_split(house, test_size = 0.3)\n",
        "X_train = train.drop(columns = ['SalePrice']).values\n",
        "y_train = train['SalePrice'].values\n",
        "X_test = test.drop(columns = ['SalePrice']).values\n",
        "y_test = test['SalePrice'].values"
      ],
      "metadata": {
        "id": "_rDZlHATpj-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we fit our random forest. We assume 500 individual trees, and for the distance $d$ we will assume the absolute error (so not the squared error). However, this would be easy to change. (Note: The fitting takes a bit.)"
      ],
      "metadata": {
        "id": "6hz0mMjFqWMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house_rf = RandomForestRegressor(n_estimators=500, random_state=1, criterion=\"absolute_error\")\n",
        "house_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-h45cgyoqZbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out the model's performance in the context of the test set:"
      ],
      "metadata": {
        "id": "xM-WdBmmryeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house_rf_test_pred = house_rf.predict(X_test)\n",
        "print(\"R2 (explained variance):\", round(metrics.r2_score(y_test, house_rf_test_pred), 2))\n",
        "print(\"Mean Absolute Perc Error (Œ£(|y-pred|/y)/n):\", round(np.mean(np.abs((y_test-house_rf_test_pred)/house_rf_test_pred)), 2))\n",
        "print(\"Mean Absolute Error (Œ£|y-pred|/n):\", \"{:,.0f}\".format(metrics.mean_absolute_error(y_test, house_rf_test_pred)))\n",
        "print(\"Root Mean Squared Error (sqrt(Œ£(y-pred)^2/n)):\", \"{:,.0f}\".format(np.sqrt(metrics.mean_squared_error(y_test, house_rf_test_pred))))"
      ],
      "metadata": {
        "id": "IRExOMZUsCqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So our pseud-R-squared is 87% and we on average are only roughly 15.5K off with our house price prediction. So our predictive model works fairly well!\n",
        "\n",
        "We could now vary e.g. the number of trees (500 above) or change other model parameters, refit, and see if the error goes down. This is how we can *tune* the model."
      ],
      "metadata": {
        "id": "YswealmQsPDM"
      }
    }
  ]
}